---
title: "Assignment 4"
output:
  pdf_document: default
  html_document: 
    keep_md: yes
editor_options:
  chunk_output_type: inline
---



# Exercise 1

## Task 1

Basis of step functionss

```{r eval=FALSE, include=FALSE}
# not run
age <- ISLR::Wage$age

```


```{r}

#' @param `x` "vector" a continuous variable
#' @param `K` "constant"
region_matrix <- function(x,K) {
  
  clevels <- levels(cut(x,K))
  binmat <- cbind(
    a = as.numeric(sub("\\((.+),.*","\\1",clevels)),
    b = as.numeric(sub("[^,]*,([^]]*)\\]","\\1",clevels))
  )
  return(as.data.frame(binmat))
}

```

```{r}

I1 <- function(x,K) {
  n <- length(x)
  a <- min(x)
  K <- K+1
  interval <- range(x)[2]-range(x)[1]
  k <- ceiling(interval / K)

  m <- matrix(0,n,K)
  
  for(j in 1:K) {
    b <- ceiling(a+k)
    
    m[,j] <- a <= x & x < b
    a <- b
  }
  
  return(
    as.matrix(m)
  )
}

cut_2 <- function(x,K) {
  n <- length(x)
  K <- K+1

  r <- region_matrix(x,K)
  m <- matrix(0,n,K)
  
  for(j in 1:K) {
    a <- r$a[j]
    b <- r$b[j]
    
    m[,j] <- x < b & x >= a
  }
  
  return(
    list(
      c_matrix = as.matrix(m),
      c_regions = r
    )
  )
}

#' 
#' @param `x` vector of obs
#' @param `K` number of steps
stepfun <- function(x,K) {
  
  X <- cut_2(x,K)$c_matrix
  y <- x
  
  #
  Intervals <- cut_2(x,K)$c_regions
  names(Intervals) <- c('Lower','Upper')
  # linear solution
  weights <- solve(t(X)%*%X)%*%t(X)%*%y
  
  # step <- lm(x~,ISLR::Wage)
  return (
    # matrix of step functions
    structure(
      list(
        intervals = as.data.frame(Intervals),
        weights = t(weights),
        step_matrix = as.matrix(X)
      )
    )
  )
}
```

```{r}
stepfun(age,5)
# lm(age~cut(age,5+1))
```

## Task 2

Hierarchical clustering

```{r eval=FALSE, include=FALSE}
## not run
X=mtcars
plot.new()
plot(hclust(dist(X)))

```

```{r}

#' Calculate the pairwise dissimilarity matrix
#' @return `data.frame` object containing pairwise dissimilarity matrix
pd <- function(X) 
{
  N <- nrow(X)
  
  # negative row indices for building merge table
  rownames(X) <- 1:N*-1
  
  # Distance matrix with 
  # upper and lower
  x <- as.data.frame(as.matrix(dist(X)))
  
  # Set diag to inf
  diag(x) <- Inf
  return(
    x
  )
}

#' Perform hierarchical clustering algorithm
#' @param Matrix X of n obs and p variables
#' @return object class 'hclust', including n-1 list of cluster indices
h_clust <- function(X)
{
  # pairwise dissimilarity matrix
  # Euclidian distance
  x <- pd(X)
  
  N <- nrow(x)
  N_1 <- N-1 

  # Initialise objects
  merge <- matrix(0,N_1,2)
  height <- vector(length=N_1)
  clusters <- lapply(1:N_1,function(x) x)
  
  for(m in 1:N_1) {
  # for(m in 1:2) {
    cols <- colnames(x)
    
    # find pair of clusters of least dissimilar
    cl <- which(x==min(x),arr.ind=TRUE)[1,,drop=FALSE]
    
    # record the pair of clusters
    merge[m,] <- as.numeric(cols[cl])
    
    # merge the pair
    l <- apply(x[cl,],2,max)
    x[min(cl),] <- l
    x[,min(cl)] <- l
    
    # 
    x[cl] <- Inf
    x[max(cl),] <- Inf
    x[,max(cl)] <- Inf
    
    cluster <- c(
      cl,
      which(cols %in% cols[cl[1,cols[cl[1]] > 0]])
    )
    
    # record cluster
    colnames(x)[cluster] <- m
    
    clusters[[m]] <- cluster
    # clusters[[m]] <- unique(clusters[[m]])
    
    # height of dendogram at fusion point
    height[m] <- min(x)
    
  }
  return(
    structure(
      list(
        merge=merge,
        height=height,
        clusters=clusters
      ),
      class='hclust'
    )
  )
}
```



```{r}

h_clust(X)$clusters
```

## Task 3

Linear aggregation of M classifiers

Bagging?

```{r}
M <- 1000
w <- runif(M)
response <- rep(0,M)
response[w > .7] <- 1

input <- w
boxplot(input~response)
```

credit score and default

leading economic indicator and interest rate change

barometer and rainfall event

## Task 4

```{r}
Smarket <- ISLR::Smarket
x <- Smarket$Today
Y <- Smarket$Direction

#' Prior probabilites
#' @param A vector containing the qualitvative response variable
#' @return A length K vector containing the prior probabilities
.pi_k <- function(Y)
{
  priors <- as.numeric(prop.table(table(Y)))
  names(priors) <- levels(Y)
  return(
    structure(
    .Data=priors
    )
  )
}

#' @return 
.mk <- function(x,Y)
{
  mu <- as.numeric(tapply(x,Y,mean))
  names(mu) <- levels(Y)
  return(
    structure(
      .Data=mu
    )
  )
}

#' @return A n*K covariance matrix 
.Sk <- function(x,Y)
{
  S <- as.numeric(tapply(x,Y,var))
  names(S) <- levels(Y)
  return(
    structure(
      .Data=S
    )
  )
}

#' Object containing the QDA k parameters
#' @return list of parameters `mu_k`, `Sigma_k`, and prior probabilities
pars <- function(x,Y)
{
  structure (
    list(
      prior=.pi_k(Y),
      mean=.mk(x,Y),
      variance=.Sk(x,Y)
    )
  )
}


quadratic_df <- function(x,pars)
{
  V <- t(as.data.frame(pars))
  K <- dim(V)[2]
  n <- length(x)
  d <- as.data.frame(matrix(0,n,K))

  p <- V[1,] #prior
  m <- V[2,] #mean
  S <- V[3,] #variance
  
  # the magic
  pk <- function(k)
  {
    t1 <- -(x^2/(2*S[k]))
    t2 <- (x*(m[k]/S[k]))
    t3 <- -(m[k]^2/(2*S[k]))
    t4 <- log(p[k])
    t5 <- -log(S[k])
    
    return(exp(t1+t2+t3+t4+t5))
  }
  
  d <- sapply(1:K,function(k) pk(k))
  class <- apply(d,1,function(x) 
    {
      colnames(V)[which.max(x)]
    })
  
  colnames(d) <- colnames(V)
  rownames(d) <- class
  return(
    structure(
      .Data = d
    )
  )
}

posteriors <- quadratic_df(x,pars(x,Y))
qda_pred <- MASS::qda(Y~x,CV=TRUE)$class
```

```{r}
predict <- posterior[,apply(posterior,1,which.max)]
```

# Exercise 2

k-fold cross validation

```{r}
#' 
#' @param d data matrix n * 2
#' @param k number of k-fold intervals
c_k = function(d,k) {
  
  # Get size of n -- num rows from d
  n = dim(d)[1]
  
  # Input var X 
  X = d[,1]
  # Response var Y
  Y = d[,2]
  
  # k-folds
  kf = 10
  
  # int vector ck length kf
  # initialised with zeros
  ck = rep(0,kf)
  
  # For each ith fold
  # estimate k-fold train MSE
  for (i in 1:kf) {
    
    # lower bound fold
    ii = ceiling(1+n*(i-1)/kf)
    
    # upper bound fold
    ii2 = ceiling(n*i/kf)
    
    # train subset
    tt = ii:ii2
    
    # index of 
    # train fold - out of sample
    tr = setdiff(1:n,tt)
    
    # Bayes post estimate
    bh = sum(X[tr]*Y[tr]/sum(X[tr]^2))
    
    yh = X[tt]*bh
    
    # ith C_k=kf
    # test mean square error
    ck[i] = mean((Y[tt] - yh)^2)
  }
  return(mean(ck))
}

# Vector of random normal 
X <- rnorm(20,0,1)
# Vector of 
Y <- 1+2*X+rnorm(20,0,2)
m <- matrix(c(X,Y),nrow=20)
c_k(m)
```


# Exercise 3

```{r}
X <- runif(10)
Z <- runif(10)
Y <- X-Z



```
